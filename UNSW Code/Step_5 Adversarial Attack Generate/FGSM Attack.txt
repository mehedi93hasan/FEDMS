import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical
import warnings
warnings.filterwarnings('ignore')

class FGSMAttacker:
    """
    Fast Gradient Sign Method (FGSM) Attack Implementation
    """

    def __init__(self, model, loss_function='categorical_crossentropy'):
        """
        Initialize FGSM attacker

        Args:
            model: Target neural network model
            loss_function: Loss function to use for gradient computation
        """
        self.model = model
        self.loss_function = loss_function
        print("🎯 FGSM Attacker initialized")

    def generate_adversarial_examples(self, x_samples, y_true, epsilon, targeted=False, target_class=None):
        """
        Generate adversarial examples using FGSM

        Args:
            x_samples: Input samples (numpy array)
            y_true: True labels (one-hot encoded)
            epsilon: Perturbation magnitude
            targeted: Whether to perform targeted attack
            target_class: Target class for targeted attack

        Returns:
            x_adversarial: Adversarial examples
            perturbations: Applied perturbations
        """
        x_tensor = tf.Variable(x_samples.astype(np.float32))

        if targeted and target_class is not None:
            # Targeted attack - create target labels
            y_target = np.zeros_like(y_true)
            y_target[:, target_class] = 1
            y_target_tensor = tf.constant(y_target.astype(np.float32))
        else:
            # Untargeted attack - use true labels
            y_target_tensor = tf.constant(y_true.astype(np.float32))

        with tf.GradientTape() as tape:
            tape.watch(x_tensor)
            predictions = self.model(x_tensor)

            if self.loss_function == 'categorical_crossentropy':
                loss = tf.keras.losses.categorical_crossentropy(y_target_tensor, predictions)
            elif self.loss_function == 'sparse_categorical_crossentropy':
                loss = tf.keras.losses.sparse_categorical_crossentropy(y_target_tensor, predictions)
            else:
                loss = tf.keras.losses.mean_squared_error(y_target_tensor, predictions)

        # Calculate gradients
        gradients = tape.gradient(loss, x_tensor)

        # Apply FGSM
        if targeted:
            # For targeted attack, move towards target class
            signed_gradients = -tf.sign(gradients)
        else:
            # For untargeted attack, move away from true class
            signed_gradients = tf.sign(gradients)

        # Generate adversarial examples
        x_adversarial = x_tensor + epsilon * signed_gradients

        # Calculate perturbations
        perturbations = x_adversarial - x_tensor

        return x_adversarial.numpy(), perturbations.numpy()

    def generate_multiple_epsilons(self, x_samples, y_true, epsilon_percentages=[2, 5, 10, 20]):
        """
        Generate adversarial examples for multiple epsilon values

        Args:
            x_samples: Input samples
            y_true: True labels
            epsilon_percentages: List of perturbation percentages

        Returns:
            Dictionary containing adversarial examples for each epsilon
        """
        results = {}

        # Calculate data range for normalization
        data_min = np.min(x_samples)
        data_max = np.max(x_samples)
        data_range = data_max - data_min

        print(f"📊 Data range: [{data_min:.4f}, {data_max:.4f}]")
        print(f"🎯 Generating adversarial examples for {len(epsilon_percentages)} epsilon values...")

        for eps_percent in epsilon_percentages:
            # Convert percentage to actual epsilon value
            epsilon = (eps_percent / 100.0) * data_range

            print(f"\n🔸 Generating FGSM attacks with {eps_percent}% perturbation (ε={epsilon:.6f})")

            # Generate adversarial examples
            x_adv, perturbations = self.generate_adversarial_examples(
                x_samples, y_true, epsilon
            )

            # Calculate perturbation statistics
            perturbation_norm = np.linalg.norm(perturbations, axis=1)
            avg_perturbation = np.mean(perturbation_norm)
            max_perturbation = np.max(perturbation_norm)

            # Test model performance on adversarial examples
            predictions_clean = self.model.predict(x_samples, verbose=0)
            predictions_adv = self.model.predict(x_adv, verbose=0)

            # Calculate accuracy
            y_true_labels = np.argmax(y_true, axis=1)
            accuracy_clean = accuracy_score(y_true_labels, np.argmax(predictions_clean, axis=1))
            accuracy_adv = accuracy_score(y_true_labels, np.argmax(predictions_adv, axis=1))

            # Calculate attack success rate
            attack_success_rate = 1 - accuracy_adv

            results[f'{eps_percent}%'] = {
                'epsilon_percent': eps_percent,
                'epsilon_value': epsilon,
                'x_adversarial': x_adv,
                'perturbations': perturbations,
                'predictions_clean': predictions_clean,
                'predictions_adversarial': predictions_adv,
                'accuracy_clean': accuracy_clean,
                'accuracy_adversarial': accuracy_adv,
                'attack_success_rate': attack_success_rate,
                'avg_perturbation_norm': avg_perturbation,
                'max_perturbation_norm': max_perturbation
            }

            print(f"   ✓ Clean accuracy: {accuracy_clean:.4f}")
            print(f"   ✓ Adversarial accuracy: {accuracy_adv:.4f}")
            print(f"   ✓ Attack success rate: {attack_success_rate:.4f}")
            print(f"   ✓ Avg perturbation norm: {avg_perturbation:.6f}")

        return results

class KDDCupFGSMExperiment:
    """
    Complete FGSM experiment pipeline for KDD Cup dataset
    """

    def __init__(self, csv_path):
        """
        Initialize experiment with dataset

        Args:
            csv_path: Path to the balanced KDD Cup CSV file
        """
        self.csv_path = csv_path
        self.data = None
        self.X = None
        self.y = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.scaler = None
        self.label_encoder = None
        self.model = None
        self.fgsm_attacker = None

        print("🎯 KDD Cup FGSM Experiment initialized")

    def load_and_prepare_data(self):
        """Load and prepare the KDD Cup dataset"""
        print("\n📂 Loading and preparing KDD Cup dataset...")

        # Load data
        self.data = pd.read_csv(self.csv_path)
        print(f"✓ Dataset loaded: {self.data.shape}")
        print(f"✓ Columns: {list(self.data.columns)}")

        # Separate features and labels
        if 'label' in self.data.columns:
            self.X = self.data.drop('label', axis=1).values
            self.y = self.data['label'].values
        elif 'class' in self.data.columns:
            self.X = self.data.drop('class', axis=1).values
            self.y = self.data['class'].values
        else:
            # Assume last column is target
            self.X = self.data.iloc[:, :-1].values
            self.y = self.data.iloc[:, -1].values

        print(f"✓ Features shape: {self.X.shape}")
        print(f"✓ Labels shape: {self.y.shape}")
        print(f"✓ Unique classes: {np.unique(self.y)}")

        # Check class distribution
        unique, counts = np.unique(self.y, return_counts=True)
        print(f"✓ Class distribution:")
        for cls, count in zip(unique, counts):
            print(f"   {cls}: {count} ({count/len(self.y)*100:.1f}%)")

        # Encode labels
        self.label_encoder = LabelEncoder()
        y_encoded = self.label_encoder.fit_transform(self.y)
        self.y_categorical = to_categorical(y_encoded)

        print(f"✓ Labels encoded to {self.y_categorical.shape[1]} classes")

        # Split data
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            self.X, self.y_categorical, test_size=0.2, random_state=42, stratify=y_encoded
        )

        # Scale features
        self.scaler = StandardScaler()
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)

        print(f"✓ Data split - Train: {self.X_train_scaled.shape[0]}, Test: {self.X_test_scaled.shape[0]}")
        print(f"✓ Features standardized")

    def create_target_model(self, hidden_layers=[64, 32], dropout_rate=0.3):
        """
        Create target neural network model

        Args:
            hidden_layers: List of hidden layer sizes
            dropout_rate: Dropout rate for regularization
        """
        print(f"\n🧠 Creating target neural network model...")

        model = Sequential()

        # Input layer
        model.add(Dense(hidden_layers[0], activation='relu', input_shape=(self.X_train_scaled.shape[1],)))
        model.add(Dropout(dropout_rate))

        # Hidden layers
        for units in hidden_layers[1:]:
            model.add(Dense(units, activation='relu'))
            model.add(Dropout(dropout_rate))

        # Output layer
        model.add(Dense(self.y_categorical.shape[1], activation='softmax'))

        # Compile model
        model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )

        self.model = model

        print(f"✓ Model created with architecture: {hidden_layers}")
        print(f"✓ Total parameters: {model.count_params():,}")

        # Display model summary
        model.summary()

    def train_target_model(self, epochs=50, batch_size=32, validation_split=0.2):
        """Train the target model"""
        print(f"\n🚀 Training target model...")

        history = self.model.fit(
            self.X_train_scaled, self.y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=validation_split,
            verbose=1
        )

        # Evaluate on test set
        test_loss, test_accuracy = self.model.evaluate(self.X_test_scaled, self.y_test, verbose=0)

        print(f"✓ Training completed!")
        print(f"✓ Test Accuracy: {test_accuracy:.4f}")
        print(f"✓ Test Loss: {test_loss:.4f}")

        return history

    def run_fgsm_attacks(self, epsilon_percentages=[2, 5, 10, 20], sample_size=1000):
        """
        Run FGSM attacks with different perturbation levels

        Args:
            epsilon_percentages: List of perturbation percentages
            sample_size: Number of samples to attack
        """
        print(f"\n⚔️  Running FGSM attacks...")
        print(f"🎯 Perturbation levels: {epsilon_percentages}%")
        print(f"🎯 Sample size: {sample_size}")

        # Initialize FGSM attacker
        self.fgsm_attacker = FGSMAttacker(self.model)

        # Select random samples for attack
        if sample_size > len(self.X_test_scaled):
            sample_size = len(self.X_test_scaled)

        # Store indices for later use in CSV creation
        self.attack_indices = np.random.choice(len(self.X_test_scaled), sample_size, replace=False)
        X_attack = self.X_test_scaled[self.attack_indices]
        y_attack = self.y_test[self.attack_indices]

        print(f"✓ Selected {len(X_attack)} samples for attack")
        print(f"✓ Attack indices stored for CSV generation")

        # Generate adversarial examples
        self.attack_results = self.fgsm_attacker.generate_multiple_epsilons(
            X_attack, y_attack, epsilon_percentages
        )

        return self.attack_results

    def analyze_attack_results(self):
        """Analyze and display attack results"""
        print(f"\n📊 FGSM ATTACK ANALYSIS")
        print("=" * 60)

        # Create summary table
        summary_data = []
        for eps_key, results in self.attack_results.items():
            summary_data.append({
                'Perturbation': eps_key,
                'Epsilon Value': f"{results['epsilon_value']:.6f}",
                'Clean Accuracy': f"{results['accuracy_clean']:.4f}",
                'Adversarial Accuracy': f"{results['accuracy_adversarial']:.4f}",
                'Attack Success Rate': f"{results['attack_success_rate']:.4f}",
                'Avg Perturbation Norm': f"{results['avg_perturbation_norm']:.6f}"
            })

        summary_df = pd.DataFrame(summary_data)
        print(summary_df.to_string(index=False))

        return summary_df

    def visualize_results(self, save_plots=True):
        """Create visualizations of attack results"""
        print(f"\n📈 Creating visualizations...")

        # Extract data for plotting
        eps_percentages = []
        clean_accuracies = []
        adv_accuracies = []
        attack_success_rates = []
        perturbation_norms = []

        for eps_key, results in self.attack_results.items():
            eps_percentages.append(results['epsilon_percent'])
            clean_accuracies.append(results['accuracy_clean'])
            adv_accuracies.append(results['accuracy_adversarial'])
            attack_success_rates.append(results['attack_success_rate'])
            perturbation_norms.append(results['avg_perturbation_norm'])

        # Create subplots
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('FGSM Attack Results on KDD Cup Dataset', fontsize=16, fontweight='bold')

        # Plot 1: Accuracy vs Perturbation
        axes[0, 0].plot(eps_percentages, clean_accuracies, 'b-o', label='Clean Accuracy', linewidth=2)
        axes[0, 0].plot(eps_percentages, adv_accuracies, 'r-s', label='Adversarial Accuracy', linewidth=2)
        axes[0, 0].set_xlabel('Perturbation Percentage (%)')
        axes[0, 0].set_ylabel('Accuracy')
        axes[0, 0].set_title('Model Accuracy vs Perturbation Level')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # Plot 2: Attack Success Rate
        axes[0, 1].bar(eps_percentages, attack_success_rates, color='red', alpha=0.7)
        axes[0, 1].set_xlabel('Perturbation Percentage (%)')
        axes[0, 1].set_ylabel('Attack Success Rate')
        axes[0, 1].set_title('FGSM Attack Success Rate')
        axes[0, 1].grid(True, alpha=0.3)

        # Plot 3: Perturbation Norms
        axes[1, 0].plot(eps_percentages, perturbation_norms, 'g-^', linewidth=2, markersize=8)
        axes[1, 0].set_xlabel('Perturbation Percentage (%)')
        axes[1, 0].set_ylabel('Average Perturbation Norm')
        axes[1, 0].set_title('Perturbation Magnitude vs Epsilon')
        axes[1, 0].grid(True, alpha=0.3)

        # Plot 4: Accuracy Drop
        accuracy_drops = [clean - adv for clean, adv in zip(clean_accuracies, adv_accuracies)]
        axes[1, 1].bar(eps_percentages, accuracy_drops, color='orange', alpha=0.7)
        axes[1, 1].set_xlabel('Perturbation Percentage (%)')
        axes[1, 1].set_ylabel('Accuracy Drop')
        axes[1, 1].set_title('Model Performance Degradation')
        axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()

        if save_plots:
            plt.savefig('fgsm_attack_results.png', dpi=300, bbox_inches='tight')
            print("✓ Plots saved as 'fgsm_attack_results.png'")

        plt.show()

    def save_adversarial_examples(self, output_dir='adversarial_examples'):
        """Save adversarial examples to files"""
        print(f"\n💾 Saving adversarial examples to '{output_dir}'...")

        import os
        os.makedirs(output_dir, exist_ok=True)

        for eps_key, results in self.attack_results.items():
            # Save adversarial examples as numpy files
            np.save(f"{output_dir}/adversarial_examples_{eps_key.replace('%', 'percent')}.npy",
                   results['x_adversarial'])

            # Save perturbations
            np.save(f"{output_dir}/perturbations_{eps_key.replace('%', 'percent')}.npy",
                   results['perturbations'])

            print(f"✓ Saved adversarial examples for {eps_key} perturbation")

        # Save metadata
        metadata = {
            'epsilon_percentages': [results['epsilon_percent'] for results in self.attack_results.values()],
            'epsilon_values': [results['epsilon_value'] for results in self.attack_results.values()],
            'attack_success_rates': [results['attack_success_rate'] for results in self.attack_results.values()],
            'feature_names': list(self.data.columns[:-1]) if 'label' in self.data.columns else None
        }

        np.save(f"{output_dir}/metadata.npy", metadata)
        print(f"✓ Saved metadata")

    def save_adversarial_csv_files(self, output_dir='adversarial_csv'):
        """
        Save adversarial examples as CSV files with labels
        Creates separate CSV files for each perturbation level
        """
        print(f"\n📄 Creating CSV files with adversarial examples and labels...")

        import os
        os.makedirs(output_dir, exist_ok=True)

        # Get feature names from original dataset
        if 'label' in self.data.columns:
            feature_names = list(self.data.columns[:-1])
        elif 'class' in self.data.columns:
            feature_names = list(self.data.columns[:-1])
        else:
            # Generate feature names if not available
            feature_names = [f'feature_{i}' for i in range(self.X.shape[1])]

        print(f"✓ Feature names: {feature_names}")

        # Get the indices used for attack
        if hasattr(self, 'attack_indices'):
            attack_indices = self.attack_indices
        else:
            # If indices not stored, use first N samples
            attack_indices = range(len(list(self.attack_results.values())[0]['x_adversarial']))

        # Get original labels for the attacked samples
        y_original = self.y_test[attack_indices] if hasattr(self, 'attack_indices') else self.y_test[:len(list(self.attack_results.values())[0]['x_adversarial'])]

        # Convert one-hot encoded labels back to original format
        if len(y_original.shape) > 1 and y_original.shape[1] > 1:
            # One-hot encoded
            y_labels_numeric = np.argmax(y_original, axis=1)
        else:
            # Already numeric
            y_labels_numeric = y_original

        # Convert back to original string labels if label encoder was used
        if hasattr(self, 'label_encoder') and self.label_encoder is not None:
            y_labels_original = self.label_encoder.inverse_transform(y_labels_numeric)
        else:
            y_labels_original = y_labels_numeric

        print(f"✓ Processing {len(y_labels_original)} samples with labels")

        # Create CSV file for each perturbation level
        for eps_key, results in self.attack_results.items():
            print(f"\n📝 Creating CSV for {eps_key} perturbation...")

            # Get adversarial examples
            x_adversarial = results['x_adversarial']

            # Create DataFrame with adversarial features
            df_adversarial = pd.DataFrame(x_adversarial, columns=feature_names)

            # Add labels column
            df_adversarial['label'] = y_labels_original

            # Add metadata columns
            df_adversarial['perturbation_level'] = eps_key
            df_adversarial['epsilon_value'] = results['epsilon_value']
            df_adversarial['is_adversarial'] = True

            # Calculate individual perturbation norms
            perturbation_norms = np.linalg.norm(results['perturbations'], axis=1)
            df_adversarial['perturbation_norm'] = perturbation_norms

            # Add prediction information
            predictions_adv = results['predictions_adversarial']
            predicted_classes = np.argmax(predictions_adv, axis=1)

            if hasattr(self, 'label_encoder') and self.label_encoder is not None:
                predicted_labels = self.label_encoder.inverse_transform(predicted_classes)
            else:
                predicted_labels = predicted_classes

            df_adversarial['predicted_label'] = predicted_labels
            df_adversarial['prediction_confidence'] = np.max(predictions_adv, axis=1)

            # Add attack success flag
            df_adversarial['attack_successful'] = (y_labels_original != predicted_labels)

            # Save CSV file
            csv_filename = f"{output_dir}/FGSM_adversarial_examples_{eps_key.replace('%', 'percent')}.csv"
            df_adversarial.to_csv(csv_filename, index=False)

            print(f"   ✓ Saved: {csv_filename}")
            print(f"   ✓ Shape: {df_adversarial.shape}")
            print(f"   ✓ Columns: {list(df_adversarial.columns)}")
            print(f"   ✓ Attack success rate: {df_adversarial['attack_successful'].mean():.4f}")

            # Display sample of the CSV
            print(f"\n   📋 Sample data preview:")
            print(df_adversarial[['label', 'predicted_label', 'attack_successful', 'perturbation_norm']].head())

        # Create a combined summary CSV
        print(f"\n📊 Creating combined summary CSV...")

        summary_data = []
        for eps_key, results in self.attack_results.items():
            summary_data.append({
                'perturbation_level': eps_key,
                'epsilon_value': results['epsilon_value'],
                'samples_count': len(results['x_adversarial']),
                'clean_accuracy': results['accuracy_clean'],
                'adversarial_accuracy': results['accuracy_adversarial'],
                'attack_success_rate': results['attack_success_rate'],
                'avg_perturbation_norm': results['avg_perturbation_norm'],
                'max_perturbation_norm': results['max_perturbation_norm']
            })

        summary_df = pd.DataFrame(summary_data)
        summary_csv_path = f"{output_dir}/attack_summary.csv"
        summary_df.to_csv(summary_csv_path, index=False)

        print(f"✓ Summary saved: {summary_csv_path}")
        print(f"✓ Summary shape: {summary_df.shape}")

        print(f"\n✅ All CSV files created successfully in '{output_dir}' directory!")

        return summary_df

    def run_complete_experiment(self, csv_path=None):
        """Run the complete FGSM experiment pipeline"""
        print("🚀 STARTING COMPLETE FGSM EXPERIMENT")
        print("=" * 70)

        if csv_path:
            self.csv_path = csv_path

        # Step 1: Load and prepare data
        self.load_and_prepare_data()

        # Step 2: Create and train target model
        self.create_target_model()
        self.train_target_model()

        # Step 3: Run FGSM attacks
        self.run_fgsm_attacks()

        # Step 4: Analyze results
        summary_df = self.analyze_attack_results()

        # Step 5: Create visualizations
        self.visualize_results()

        # Step 6: Save results as numpy files
        self.save_adversarial_examples()

        # Step 7: Save results as CSV files (NEW)
        csv_summary_df = self.save_adversarial_csv_files()

        print(f"\n✅ EXPERIMENT COMPLETED SUCCESSFULLY!")
        print(f"📊 Results summary:")
        print(summary_df.to_string(index=False))

        print(f"\n📄 CSV Files Created:")
        print(f"   • adversarial_csv/adversarial_examples_2percent.csv")
        print(f"   • adversarial_csv/adversarial_examples_5percent.csv")
        print(f"   • adversarial_csv/adversarial_examples_10percent.csv")
        print(f"   • adversarial_csv/adversarial_examples_20percent.csv")
        print(f"   • adversarial_csv/attack_summary.csv")

        return self.attack_results, summary_df, csv_summary_df

# ===== MAIN EXECUTION =====
def main():
    """Main function to run FGSM experiment"""

    print("⚔️  FGSM ATTACK IMPLEMENTATION FOR KDD CUP DATASET")
    print("=" * 70)

    # Configuration
    CSV_PATH = "kdd_selected_features.csv"  # 🔹 UPDATE THIS PATH

    print(f"📁 Dataset path: {CSV_PATH}")
    print(f"🎯 Attack type: Fast Gradient Sign Method (FGSM)")
    print(f"🎯 Perturbation levels: 2%, 5%, 10%, 20%")

    try:
        # Initialize experiment
        experiment = KDDCupFGSMExperiment(CSV_PATH)

        # Run complete experiment
        attack_results, summary_df, csv_summary_df = experiment.run_complete_experiment()

        print(f"\n🎯 EXPERIMENT SUMMARY:")
        print(f"   • Dataset processed: ✓")
        print(f"   • Target model trained: ✓")
        print(f"   • FGSM attacks generated: ✓")
        print(f"   • Results analyzed: ✓")
        print(f"   • Visualizations created: ✓")
        print(f"   • Adversarial examples saved (NumPy): ✓")
        print(f"   • CSV files created: ✓")

        # Display key findings
        max_success_rate = max([results['attack_success_rate'] for results in attack_results.values()])
        print(f"\n🔍 KEY FINDINGS:")
        print(f"   • Maximum attack success rate: {max_success_rate:.4f}")
        print(f"   • Model vulnerability confirmed across all perturbation levels")
        print(f"   • 4 CSV files created with adversarial examples and labels")
        print(f"   • Ready for adversarial defense testing")

        print(f"\n📄 CSV FILES GENERATED:")
        print(f"   📁 adversarial_csv/")
        print(f"   ├── adversarial_examples_2percent.csv   (2% perturbation)")
        print(f"   ├── adversarial_examples_5percent.csv   (5% perturbation)")
        print(f"   ├── adversarial_examples_10percent.csv  (10% perturbation)")
        print(f"   ├── adversarial_examples_20percent.csv  (20% perturbation)")
        print(f"   └── attack_summary.csv                  (Overall summary)")

        print(f"\n💡 NEXT STEPS:")
        print(f"   1. Use these CSV files to test your adversarial defense system")
        print(f"   2. Feed adversarial examples through Algorithms 1 & 2")
        print(f"   3. Evaluate real-time adaptation effectiveness")
        print(f"   4. Compare defense performance across perturbation levels")

    except FileNotFoundError:
        print(f"❌ Error: Dataset file not found at {CSV_PATH}")
        print(f"🔧 Please update CSV_PATH with your actual dataset location")

    except Exception as e:
        print(f"❌ Error during experiment: {e}")
        print(f"\n🔧 Troubleshooting:")
        print(f"   1. Check dataset path and format")
        print(f"   2. Ensure all dependencies are installed")
        print(f"   3. Verify dataset has 'label' or 'class' column")

if __name__ == "__main__":
    main()

